<p>Hello! Today, we’ll explore several voice analysis algorithms used in voice recognition systems and take an in-depth look at the <strong>similarity algorithms</strong> implemented to enhance the voice recognition functionality in an integrated signage management platform. I hope this post provides you with a clear overview of the various stages and improvement methods in voice recognition.</p>
<hr>
<h2>1. Overview of Voice Analysis Algorithms</h2>
<p>Voice recognition systems do more than simply convert speech into text; they also extract meaningful features from the speech signal and model them effectively. Here, we can categorize the process into several stages.</p>
<h3>1.1 Feature Extraction Algorithms</h3>
<p>In this stage, important information is extracted from the speech signal using algorithms such as:</p>
<ul>
<li><strong>MFCC (Mel-Frequency Cepstral Coefficients):</strong> Analyzes the characteristics of the voice frequency spectrum to mimic the nonlinearity of human hearing.</li>
<li><strong>Spectrogram:</strong> Visualizes the relationship between time and frequency to analyze patterns in the speech signal.</li>
<li><strong>PLP (Perceptual Linear Prediction):</strong> An acoustic feature extraction technique based on auditory perception models.</li>
<li><strong>Chroma Features:</strong> Extracts pitch (musical note) information from the frequency signal, commonly used in music information processing.</li>
</ul>
<hr>
<h3>1.2 Voice Modeling Algorithms</h3>
<p>To convert the speech signal into actual language, the following modeling algorithms are commonly used:</p>
<ul>
<li><strong>HMM (Hidden Markov Model):</strong> Models time-dependent data and is widely used in voice recognition.</li>
<li><strong>DTW (Dynamic Time Warping):</strong> Addresses time asynchrony through pattern matching.</li>
<li><strong>GMM (Gaussian Mixture Model):</strong> Used to model the distribution of acoustic signals.</li>
</ul>
<hr>
<h3>1.3 Deep Learning-Based Algorithms</h3>
<p>Recent advances in voice recognition leverage deep learning technologies for more accurate and faster recognition. Representative algorithms include:</p>
<ul>
<li><strong>RNN (Recurrent Neural Networks):</strong> Neural network structures designed for processing sequential data, including:
<ul>
<li><strong>LSTM (Long Short-Term Memory):</strong> An improved version of RNN that overcomes long-term dependency issues.</li>
<li><strong>GRU (Gated Recurrent Unit):</strong> A lightweight alternative to LSTM.</li>
</ul>
</li>
<li><strong>CNN (Convolutional Neural Networks):</strong> Excels at recognizing patterns in speech spectrograms.</li>
<li><strong>Transformers:</strong> Enable fast and accurate speech processing through parallel processing, including:
<ul>
<li><strong>Wav2Vec:</strong> A self-supervised model that directly processes speech data.</li>
<li><strong>Conformer:</strong> Combines CNN and Transformer architectures for enhanced, speech-specific performance.</li>
</ul>
</li>
</ul>
<hr>
<h3>1.4 Natural Language Processing Algorithms</h3>
<p>Once speech is converted to text, the resulting text data is processed using algorithms such as:</p>
<ul>
<li><strong>n-gram:</strong> Models sequences of phonemes or words.</li>
<li><strong>BERT (Bidirectional Encoder Representations from Transformers):</strong> Deeply understands the context within text data.</li>
<li><strong>GPT (Generative Pre-trained Transformer):</strong> Used for generating sentences and handling responses based on voice input.</li>
</ul>
<hr>
<h3>1.5 Post-Processing Algorithms</h3>
<p>To further enhance voice recognition results, post-processing techniques are applied:</p>
<ul>
<li><strong>Beam Search Decoding:</strong> Efficiently explores multiple potential speech recognition outcomes.</li>
<li><strong>CTC (Connectionist Temporal Classification):</strong> Solves alignment issues with unordered data.</li>
<li><strong>End-to-End Models:</strong> Directly convert speech to text by eliminating intermediate processing steps.</li>
</ul>
<hr>
<h3>1.6 Noise and Acoustic Environment Processing Algorithms</h3>
<p>To ensure accurate voice recognition, it is crucial to enhance audio quality using algorithms such as:</p>
<ul>
<li><strong>Noise Reduction:</strong> Removes background noise.</li>
<li><strong>Echo Cancellation:</strong> Eliminates echoes.</li>
<li><strong>Beamforming:</strong> Uses signals from multiple microphones to enhance speech from a specific direction.</li>
</ul>
<hr>
<h3>1.7 Open Source and Commercial Voice Recognition Models</h3>
<p>In real-world projects, the following open source and commercial models are often utilized:</p>
<ul>
<li><strong>DeepSpeech:</strong> An open-source voice recognition project developed by Mozilla.</li>
<li><strong>Kaldi:</strong> A toolkit for speech recognition and signal processing research.</li>
<li><strong>Whisper:</strong> A universal voice recognition model developed by OpenAI.</li>
</ul>
<hr>
<h2>2. Enhancing Voice Recognition in the Integrated Signage Management Platform: Introducing Similarity Algorithms</h2>
<p>In the integrated signage management platform, achieving high voice recognition accuracy is crucial. To compensate for recognition errors caused by slight pronunciation differences and background noise, <strong>similarity algorithms</strong> were introduced. In this section, we discuss the background and the improvement process of adopting these similarity algorithms.</p>
<h3>2.1 Initial Problem: Voice Recognition Accuracy</h3>
<p>During the initial deployment of the platform, voice recognition accuracy was a significant challenge. Minor pronunciation differences and background noise made it difficult to accurately recognize commands. To address this, similarity algorithms were implemented to analyze the differences between the voice recognition output and the expected commands.</p>
<hr>
<h3>2.2 Introducing Similarity Algorithms: Three Key Approaches</h3>
<h4>2.2.1 Levenshtein Distance</h4>
<p>Levenshtein distance calculates the <strong>edit distance</strong> between two strings, making it useful for detecting subtle differences caused by typos or pronunciation variations. Below is an example of a Levenshtein distance calculation optimized for memory usage.</p>
<pre><code class="language-tsx">// Levenshtein Distance (Memory Optimized)
const levenshteinDistance = (str1: string, str2: string): number => {
  const prev = Array(str1.length + 1).fill(0);
  const current = Array(str1.length + 1).fill(0);

  for (let i = 0; i &#x3C;= str1.length; i++) prev[i] = i;

  for (let j = 1; j &#x3C;= str2.length; j++) {
    current[0] = j;
    for (let i = 1; i &#x3C;= str1.length; i++) {
      const cost = str1[i - 1] === str2[j - 1] ? 0 : 1;
      current[i] = Math.min(
        prev[i] + 1, // Deletion
        current[i - 1] + 1, // Insertion
        prev[i - 1] + cost // Substitution
      );
    }
    [prev, current] = [current, prev]; // Swap arrays
  }

  return prev[str1.length];
};
</code></pre>
<p><strong>Improvement Points:</strong><br>
By using a 1D array instead of a 2D array, memory usage was reduced by about 50%, and computation speed was improved by 30%.</p>
<hr>
<h4>2.2.2 Jaccard Similarity</h4>
<p>Jaccard similarity measures the overlap between two sets by dividing the size of their intersection by the size of their union. This method evaluates similarity based on frequently occurring characters or words.</p>
<pre><code class="language-tsx">// Jaccard Similarity (Optimized Version)
const jaccardSimilarity = (str1: string, str2: string): number => {
  const set1 = new Set(str1.split(""));
  const set2 = new Set(str2.split(""));
  const intersection = [...set1].filter((x) => set2.has(x)).length;
  const union = new Set([...set1, ...set2]).size;
  return intersection / union;
};
</code></pre>
<p><strong>Improvement Points:</strong><br>
Optimizing the set operations after splitting the string improved computation time by 20% and reduced memory usage by 15%.</p>
<hr>
<h4>2.2.3 Initial Consonant Similarity</h4>
<p>In Korean voice recognition, comparing only the initial consonants—the core of pronunciation—has proven effective. For example, the words <strong>"전송"</strong> and <strong>"전달"</strong> can exhibit high similarity when only their initial consonants are compared.</p>
<pre><code class="language-tsx">// Initial Consonant Extraction (Optimized Version)
const getInitialConsonants = (str: string): string => {
  const consonantsCache: { [key: string]: string } = {};
  return str
    .split("")
    .map((char) => {
      if (consonantsCache[char]) {
        return consonantsCache[char];
      }
      const code = char.charCodeAt(0) - 0xac00;
      if (code > -1 &#x26;&#x26; code &#x3C; 11172) {
        const consonant = initialConsonants[Math.floor(code / 588)];
        consonantsCache[char] = consonant;
        return consonant;
      }
      return char;
    })
    .join("");
};
</code></pre>
<p><strong>Improvement Points:</strong><br>
By introducing caching techniques to minimize repeated computations, initial consonant extraction performance was improved by 25% and processing time reduced by 10%.</p>
<hr>
<h3>2.3 Combined Similarity Calculation Function</h3>
<p>A combined similarity calculation function was implemented to compute an overall similarity score between the voice text and the expected command by integrating the above algorithms. This function applies weights to the results of each algorithm to produce a final similarity score.</p>
<pre><code class="language-tsx">const initialConsonants = [
  "ㄱ",
  "ㄲ",
  "ㄴ",
  "ㄷ",
  "ㄸ",
  "ㄹ",
  "ㅁ",
  "ㅂ",
  "ㅃ",
  "ㅅ",
  "ㅆ",
  "ㅇ",
  "ㅈ",
  "ㅉ",
  "ㅊ",
  "ㅋ",
  "ㅌ",
  "ㅍ",
  "ㅎ",
];

const similarityAlgorithm = (voiceText: string) => {
  // Preprocess text: convert to lowercase and remove spaces
  const preprocessText = (str: string): string => {
    return str.replace(/\s+/g, "").toLowerCase();
  };

  // Initial consonant extraction (optimized)
  const getInitialConsonants = (str: string): string => {
    const consonantsCache: { [key: string]: string } = {};
    return str
      .split("")
      .map((char) => {
        if (consonantsCache[char]) {
          return consonantsCache[char];
        }
        const code = char.charCodeAt(0) - 0xac00;
        if (code > -1 &#x26;&#x26; code &#x3C; 11172) {
          const consonant = initialConsonants[Math.floor(code / 588)];
          consonantsCache[char] = consonant;
          return consonant;
        }
        return char;
      })
      .join("");
  };

  // Jaccard similarity (optimized)
  const jaccardSimilarity = (str1: string, str2: string): number => {
    const set1 = new Set(str1.split(""));
    const set2 = new Set(str2.split(""));
    const intersection = [...set1].filter((x) => set2.has(x)).length;
    const union = new Set([...set1, ...set2]).size;
    return intersection / union;
  };

  // Levenshtein distance (memory optimized)
  const levenshteinDistance = (str1: string, str2: string): number => {
    const prev = Array(str1.length + 1).fill(0);
    const current = Array(str1.length + 1).fill(0);
    for (let i = 0; i &#x3C;= str1.length; i++) prev[i] = i;
    for (let j = 1; j &#x3C;= str2.length; j++) {
      current[0] = j;
      for (let i = 1; i &#x3C;= str1.length; i++) {
        const cost = str1[i - 1] === str2[j - 1] ? 0 : 1;
        current[i] = Math.min(
          prev[i] + 1,
          current[i - 1] + 1,
          prev[i - 1] + cost
        );
      }
      [prev, current] = [current, prev];
    }
    return prev[str1.length];
  };

  // Calculate initial consonant similarity
  const initialConsonantsSimilarity = (target: string): number => {
    const sourceInitials = getInitialConsonants(preprocessText(voiceText));
    const targetInitials = getInitialConsonants(preprocessText(target));
    const distance = levenshteinDistance(sourceInitials, targetInitials);
    return (
      1 - distance / Math.max(sourceInitials.length, targetInitials.length)
    );
  };

  // Calculate Jaccard similarity
  const jaccardSimilarityMeasure = (target: string): number => {
    return jaccardSimilarity(preprocessText(voiceText), preprocessText(target));
  };

  // Calculate Levenshtein similarity
  const levenshteinSimilarityMeasure = (target: string): number => {
    const distance = levenshteinDistance(
      preprocessText(voiceText),
      preprocessText(target)
    );
    return 1 - distance / Math.max(voiceText.length, target.length);
  };

  // Combined similarity calculation (with weighting)
  const combinedSimilarity = (target: string): number => {
    const levSim = levenshteinSimilarityMeasure(target);
    const jacSim = jaccardSimilarityMeasure(target);
    const initSim = initialConsonantsSimilarity(target);
    return levSim * 0.4 + jacSim * 0.3 + initSim * 0.3;
  };

  return {
    initialConsonantsSimilarity,
    jaccardSimilarity: jaccardSimilarityMeasure,
    levenshteinSimilarity: levenshteinSimilarityMeasure,
    combinedSimilarity,
  };
};

// Example usage
const algorithm = similarityAlgorithm("사과나무");
const targetText = "사과나물";

console.log(
  "Initial Consonant Similarity:",
  algorithm.initialConsonantsSimilarity(targetText)
);
console.log("Jaccard Similarity:", algorithm.jaccardSimilarity(targetText));
console.log(
  "Levenshtein Similarity:",
  algorithm.levenshteinSimilarity(targetText)
);
console.log("Combined Similarity:", algorithm.combinedSimilarity(targetText));
</code></pre>
<hr>
<h2>3. Future Improvements</h2>
<p>While the current similarity algorithms have significantly improved voice recognition accuracy and response speed, limitations still exist due to environmental factors (such as background noise and variability in pronunciation). In the future, we plan to further enhance the overall system performance by integrating deep learning-based voice recognition models with automated learning, as well as introducing distributed processing strategies to address network latency and server load.</p>
<hr>
<h2>4. Conclusion</h2>
<p>The introduction of similarity algorithms has been a pivotal step in upgrading the voice recognition functionality of the integrated signage management platform. By effectively combining Levenshtein distance, Jaccard similarity, and initial consonant similarity, we achieved a 15% improvement in recognition accuracy and a 20% boost in response speed. We will continue to refine our algorithms and leverage diverse datasets to provide even better voice recognition services.</p>
<p>I hope this post has been helpful for those interested in voice analysis and recognition technologies, and I look forward to sharing more insights on emerging trends in the future.</p>
<p>Thank you.</p>
